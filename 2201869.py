# -*- coding: utf-8 -*-
"""2201869.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sBRvptTEnBi_D4F1hrEJiPgC-98y4jL
"""

import os
import pandas as pd
import numpy as np
import multiprocessing
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error, f1_score, precision_score, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

"""Add time to each data set and need to match with button click event on tag file"""

# Add time spam to each data set

def add_timespamtodataset(df,count):

  df_timestamp = df.iloc[0, 0]
  df_rate = df.iloc[1, 0]  
  #print(df_timestamp)
  #print(df_rate)
  n_df = pd.DataFrame(df.iloc[2:].values, columns=df.columns)  
  n_df['id'] = count 
  n_df['timespam'] = [(df_timestamp + i / df_rate) for i in range(len(n_df))]
    
  return n_df

# Document Name as array

document_name= ["ACC","BVP","EDA","HR","TEMP"] 
document_name=np.asarray(document_name)

#Define dataframe for each document

df_acc  =""
df_tags =""
df_bvp  =""
df_eda  =""
df_hr   =""
df_ibi  ="" 
df_temp =""

# skipthe data set 1. all the data not available in same time spam

for x in range(2,36):   #chamge the value and select some files Otherwise data loading take more time
    #print(x) 
    for i in document_name:
        #print(i)
        
        file_path ="https://raw.githubusercontent.com/italha-d/Stress-Predict-Dataset/main/Raw_data/"
        
        if i == "tags" :
            
            if x > 9:
                
                file_path = file_path + "S" + str(x) + "/" + i + "_S" + str(x) + ".csv"
                
            else:
                
                file_path = file_path + "S" + ("0" + str(x)) +"/" + i + "_S" + ("0" + str(x)) +".csv"            
             
            if len(df_tags) == 0:
                
                df_tags = pd.read_csv(file_path, on_bad_lines='skip', header = None , skiprows=[0,1])
                
            else :
                # Concat all the data into one column with axis =0
                df_tags= pd.concat([ pd.read_csv(file_path, on_bad_lines='skip' , header= None ) , df_tags ] ,axis=0 )           
                     
        else :                      
                
            if x > 9: 
                
                file_path = file_path + "S" + str(x) + "/" + i +".csv"
                
            else :
                
                file_path = file_path + "S" + ("0" + str(x)) + "/" + i + ".csv"
            
            if i == "ACC" :
                if len(df_acc) == 0:
                    names = ['X','Y','Z']
                    df_acc = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names )                     
                    df_acc = add_timespamtodataset(df_acc , x )                   
                    
                else :
                    
                    names = ['X','Y','Z']
                    df_other = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names )                                
                    df_other = add_timespamtodataset(df_other , x )
                    df_acc= pd.concat ([ df_other , df_acc ]  ,axis=0  )                    
                     
            if i == "BVP" :                
                if len(df_bvp) == 0:
                    
                    names = ['BVP']
                    df_bvp = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names )
                    df_bvp = add_timespamtodataset(df_bvp , x )                                           
                     
                else :     
                    names = ['BVP']
                    df_other = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names  )
                    df_other = add_timespamtodataset(df_other , x )
                    df_bvp= pd.concat( [ df_other , df_bvp ] ,axis=0 )                     
                     
            if i == "EDA" :
                if len(df_eda) == 0:
                    
                    names = ['EDA']
                    df_eda = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names)
                    df_eda = add_timespamtodataset(df_eda , x )  
                    
                else :
                    
                    names = ['EDA']
                    df_other = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names  )
                    df_other = add_timespamtodataset(df_other , x )
                    df_eda= pd.concat ([ df_other , df_eda] ,axis=0 )
                    
            if i == "HR" :                
                if len(df_hr) == 0:
                    
                    names = ['HR']
                    df_hr = pd.read_csv(file_path, on_bad_lines='skip', header= None , names=names)
                    df_hr = add_timespamtodataset(df_hr , x )  
                    
                else :                    
                    names = ['HR']
                    
                    df_other = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names  )
                    df_other = add_timespamtodataset(df_other , x )
                    df_hr= pd.concat ([ df_other , df_hr] ,axis=0 )
                    
                    
            # print(file_path)
            if i == "TEMP" :
                
                if len(df_temp) == 0:
                    
                    names = ['TEMP']
                    df_temp = pd.read_csv(file_path, on_bad_lines='skip', header= None , names=names )
                    df_temp = add_timespamtodataset(df_temp , x )
                    
                else :
                    
                    names = ['TEMP']
                    df_other = pd.read_csv(file_path, on_bad_lines='skip' , header= None , names=names  )
                    df_other = add_timespamtodataset(df_other , x )
                    df_temp= pd.concat ([ df_other , df_temp] ,axis=0 )
                    
print('All the Data sets Imported')

#  Merge Data Set into single data frame

def merge_dataset(x):
    
    columns=['X', 'Y', 'Z','BVP', 'EDA', 'HR', 'TEMP',  'timespam']
    
    df_merge=pd.DataFrame(columns=columns)

    bvp_id = df_bvp[df_bvp['id'] == x]
    acc_id = df_acc[df_acc['id'] == x].drop(['id'], axis=1)
    eda_id = df_eda[df_eda['id'] == x].drop(['id'], axis=1)
    hr_id = df_hr[df_hr['id'] == x].drop(['id'], axis=1)
    temp_id = df_temp[df_temp['id'] == x].drop(['id'], axis=1)

     
    df_merge = pd.merge(acc_id, bvp_id, on='timespam'  , how='outer')
    df_merge = pd.merge(df_merge, eda_id, on='timespam'  , how='outer')
    df_merge = pd.merge(df_merge, hr_id, on='timespam'  , how='outer')
    df_merge = pd.merge(df_merge, temp_id, on='timespam'  , how='outer')

    df_merge.fillna(method='ffill', inplace=True) # fill NAN by foarward filling method
    df_merge.fillna(method='bfill', inplace=True) # fill NAN by backward filling method
    
    
    return df_merge

# Merger all the data into one single data frame and remove the NAN values by forward filling method and backward filling method

merge_df = pd.DataFrame()

for i in range(2,36):

    if len(merge_df) == 0:
       merge_df = merge_dataset(i)

    else:
       merge_df = pd.concat([merge_df, merge_dataset(i)], axis=0 )
print('Data set merged')

merge_df['Label'] = pd.Series(dtype='int') # Add label column to merge data set to match the output 1 or 0 according to the stress
print(merge_df)

# Get the all data into different data frames and analyse the data for all the subjects 

df_2 =merge_df.loc[merge_df['id'] == 2]
df_3 =merge_df.loc[merge_df['id'] == 3]
df_4 =merge_df.loc[merge_df['id'] == 4]
df_5 =merge_df.loc[merge_df['id'] == 5]
df_6 =merge_df.loc[merge_df['id'] == 6]
df_7 =merge_df.loc[merge_df['id'] == 7]
df_8 =merge_df.loc[merge_df['id'] == 8]
df_9 =merge_df.loc[merge_df['id'] == 9]
df_10 =merge_df.loc[merge_df['id'] == 10]
df_11 =merge_df.loc[merge_df['id'] == 11]
df_12 =merge_df.loc[merge_df['id'] == 12]
df_13 =merge_df.loc[merge_df['id'] == 13]
df_14 =merge_df.loc[merge_df['id'] == 14]
df_15 =merge_df.loc[merge_df['id'] == 15]
df_16 =merge_df.loc[merge_df['id'] == 16]
df_17 =merge_df.loc[merge_df['id'] == 17]
df_18 =merge_df.loc[merge_df['id'] == 18]
df_19 =merge_df.loc[merge_df['id'] == 19]
df_20 =merge_df.loc[merge_df['id'] == 20]
df_21 =merge_df.loc[merge_df['id'] == 21]
df_22 =merge_df.loc[merge_df['id'] == 22]
df_23 =merge_df.loc[merge_df['id'] == 23]
df_24 =merge_df.loc[merge_df['id'] == 24]
df_25 =merge_df.loc[merge_df['id'] == 25]
df_26 =merge_df.loc[merge_df['id'] == 26]
df_27 =merge_df.loc[merge_df['id'] == 27]
df_28 =merge_df.loc[merge_df['id'] == 28]
df_29 =merge_df.loc[merge_df['id'] == 29]
df_30 =merge_df.loc[merge_df['id'] == 30]
df_31 =merge_df.loc[merge_df['id'] == 31]
df_32 =merge_df.loc[merge_df['id'] == 32]
df_33 =merge_df.loc[merge_df['id'] == 33]
df_34 =merge_df.loc[merge_df['id'] == 34]
df_35 =merge_df.loc[merge_df['id'] == 35]

# Function for remove outliers by hampel filtering method

def hampel_filter(df, column, window_size=3, n_sigma=3):
   
    rolling_median = df[column].rolling(window_size, center=True).median()
    deviation = np.abs(df[column] - rolling_median)
    median_deviation = deviation.rolling(window_size, center=True).median()
    threshold = n_sigma * median_deviation
    outlier_idx = deviation > threshold
    df.loc[outlier_idx, column] = rolling_median[outlier_idx]
    return df

# match output to the lable column according to the time tag values as 0 and 1 
def match_label(df,x):

  file_path=''
  

  file_path = "https://raw.githubusercontent.com/italha-d/Stress-Predict-Dataset/main/Raw_data/"

  if x > 9:
                
    file_path = file_path + "S" + str(x) + "/" + 'tags' + "_S" + str(x) + ".csv"
  else:
            
    file_path = file_path + "S" + ("0" + str(x)) +"/" + 'tags' + "_S" + ("0" + str(x)) +".csv"         
          
  df_tags = pd.read_csv(file_path, on_bad_lines='skip', header = None )
  print(file_path) 
  tag = df_tags.values 
  value_if_true =1
  value_if_false =0 
  #print(tag)
  for i in range(len(tag)):
     
    if i == 0:     

      df.loc[(df["id"].values == x) & (df['timespam'].values < tag[i]) , "Label"] =0
      
      df.loc[ (df["id"].values == x) & (df['timespam'].values >= tag[i]) & (df['timespam'].values < tag[i+1]) , "Label"] =value_if_true
      
    else:
      if i == len(tag)-1:
        
        df.loc[(df["id"].values == x) & (df['timespam'].values >= tag[i])  , "Label"] = value_if_true
        
      else :
        
        df.loc[ (df["id"].values == x) & (df['timespam'].values >= tag[i]) & (df['timespam'].values < tag[i+1]) , "Label"] =value_if_true
        

      if value_if_true ==1:
        value_if_true =0
        value_if_false =1
      else:
        value_if_true =1
        value_if_false=0
    
  #print(dfname)
  #filename =filename+'.csv'
  #dfname.to_csv(filename)

print('Label Matching Completed')

# Function for detect the outliers and lable matching

document_name= ["X","Y","Z","BVP","EDA","HR","TEMP"] 
document_name=np.asarray(document_name)

for x in range(2,36):
  dfname ='df_' + str(x)
  df = globals()[dfname]

  for i in document_name:
    df= hampel_filter(df,str(i),3,3)

  match_label(df,x)

# function for plot the data

def plot(df):
  fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 5))
  fig.tight_layout(pad=2.0)
  ax[0,0].set_title('X')
  ax[0,0].plot( df['timespam'],df['X'], label='Line 2')
  ax[0,1].set_title('Y')
  ax[0,1].plot( df['timespam'],df['X'], label='Line 2')
  ax[0,2].set_title('Z')
  ax[0,2].plot( df['timespam'],df['Z'], label='Line 2')
  ax[1,0].set_title('BVP')
  ax[1,0].plot( df['timespam'],df['BVP'], label='Line 2')
  ax[1,1].set_title('HR')
  ax[1,1].plot( df['timespam'],df['HR'], label='Line 2')
  ax[1,2].set_title('TEMP')
  ax[1,2].plot( df['timespam'],df['TEMP'], label='Line 2')

  #plt.show()

data = {'ID': [],'Subject_ID': [],'MSE': [], 'F1 Score': [], 'Precision': [], 'Accuracy': [],'Accuracy Scores':[],'Mean Accuracy':[]}

# function for train the model and analyse the perpormance evaluation of the model. I have get the cross validation scores also here

def train_randomforestmodel(df, subject_id,id):
 
  X= df.iloc[:,:-1]
  Y= df.iloc[:,-1]

   
  # split the data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

  # train a random forest model on the training data
  rf = RandomForestClassifier(n_estimators=100)
  rf.fit(X_train, y_train)

  # get the cross validation scores
  scores = cross_val_score(rf, X, Y, cv=10)

  # make predictions on the test data
  y_pred = rf.predict(X_test)

  # Generate the   feature importances
  feature_importances = rf.feature_importances_

  # create a dataframe to store the feature importances
  importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

  # sort the dataframe by feature importance in descending order
  importance_df = importance_df.sort_values(by='Importance', ascending=False)

  print ('feature importances')
  print (importance_df)

  # Plot a bar graph of the feature importances
  plt.figure(figsize=(6, 4))

  plt.bar(importance_df["Feature"], importance_df["Importance"] , width=0.3)
  plt.xticks(rotation=90)
  plt.xlabel("Feature")
  plt.ylabel("Importance")
  plt.title("Random Forest Feature Importances")
  plt.show()


  print('')
  # calculate the mean squared error
  mse = mean_squared_error(y_test, y_pred)

  # calculate the F1 score
  f1 = f1_score(y_test, y_pred)

  # calculate the precision
  precision = precision_score(y_test, y_pred)

  # calculate the accuracy
  accuracy = accuracy_score(y_test, y_pred)

  # calculate the confusion matrix
  cm = confusion_matrix(y_test, y_pred)
  data['ID'].append(id)
  data['Subject_ID'].append(subject_id)
  data['MSE'].append(mse)
  data['F1 Score'].append(f1)
  data['Precision'].append(precision)
  data['Accuracy'].append(accuracy)
  data['Accuracy Scores'].append(scores)
  data['Mean Accuracy'].append(scores.mean())
  # print the results
  print('Accuracy Scores: ', scores)
  print('Mean Accuracy: ', scores.mean())
  print("Mean squared error:", mse)
  print("F1 score:", f1)
  print("Precision:", precision)
  print("Accuracy:", accuracy)
  print("Confusion matrix:")
  print(cm)

# function for train by BVP and HR under feature importance

def train_randomforestmodel_withBVP(df, subject_id,id):
 
  X = df[['BVP', 'HR']]
  Y = df['Label']

   
  # split the data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

  # train a random forest model on the training data
  rf = RandomForestClassifier(n_estimators=100)
  rf.fit(X_train, y_train)

  # get the cross validation scores
  scores = cross_val_score(rf, X, Y, cv=5)

  # make predictions on the test data
  y_pred = rf.predict(X_test)

  # Generate the   feature importances
  feature_importances = rf.feature_importances_

  # create a dataframe to store the feature importances
  importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

  # sort the dataframe by feature importance in descending order
  importance_df = importance_df.sort_values(by='Importance', ascending=False)

  print ('feature importances')
  print (importance_df)

  # Plot a bar graph of the feature importances
  plt.figure(figsize=(6, 4))

  plt.bar(importance_df["Feature"], importance_df["Importance"] , width=0.3)
  plt.xticks(rotation=90)
  plt.xlabel("Feature")
  plt.ylabel("Importance")
  plt.title("Random Forest Feature Importances")
  plt.show()


  print('')
  # calculate the mean squared error
  mse = mean_squared_error(y_test, y_pred)

  # calculate the F1 score
  f1 = f1_score(y_test, y_pred)

  # calculate the precision
  precision = precision_score(y_test, y_pred)

  # calculate the accuracy
  accuracy = accuracy_score(y_test, y_pred)

  # calculate the confusion matrix
  cm = confusion_matrix(y_test, y_pred)
  data['ID'].append(id)
  data['Subject_ID'].append(subject_id)
  data['MSE'].append(mse)
  data['F1 Score'].append(f1)
  data['Precision'].append(precision)
  data['Accuracy'].append(accuracy)
  data['Accuracy Scores'].append(scores)
  data['Mean Accuracy'].append(scores.mean())
  # print the results
  print('Accuracy Scores: ', scores)
  print('Mean Accuracy: ', scores.mean())
  print("Mean squared error:", mse)
  print("F1 score:", f1)
  print("Precision:", precision)
  print("Accuracy:", accuracy)
  print("Confusion matrix:")
  print(cm)

# function for analyse the data . Call the plot function and train model function
def analyse_data(df,subject_id,id):
  #df = df.drop("Unnamed: 0", axis=1)
  plot(df)
  print('')

  df = df.drop(columns=["id", "timespam"])

  corr= df.corr()   
  print(corr['Label'])
  print('')

  train_randomforestmodel(df,subject_id,id)
  print('')

 # df = df.drop(['datetime','id' ] )
  plt.figure(figsize=(4, 3))
  df["Label"].value_counts()
  df["Label"].value_counts().plot(kind="bar",color=["salmon","deeppink"]) 
  
  plt.xticks(np.arange(2), ('Stressed', 'Not Stressed'),rotation=0 );

analyse_data(df_2, 'S2',2)

train_randomforestmodel_withBVP(df_2, 'S2',2)

analyse_data(df_3,'S3',3)

analyse_data(df_4,'S4',4)

print(data)

analyse_data(df_5,'S5',5)

analyse_data(df_6,'S6',6)

analyse_data(df_7,'S7',7)

analyse_data(df_8,'S8',8)

analyse_data(df_9,'S9',9)

analyse_data(df_10,'S10',10)

analyse_data(df_11,'S11',11)

#@title Default title text
analyse_data(df_12,'S12',12)

analyse_data(df_13,'S13',13)

analyse_data(df_14,'S14',14)

analyse_data(df_15,'S15',15)

analyse_data(df_16,'S16',16)

analyse_data(df_17,'S17',17)

analyse_data(df_18,'S18',18)

analyse_data(df_19,'S19',19)

analyse_data(df_20,'S20',20)

analyse_data(df_21,'S21',21)

analyse_data(df_22,'S22',22)

analyse_data(df_23,'S23',23)

analyse_data(df_24,'S24',24)

analyse_data(df_25,'S25',25)

analyse_data(df_26,'S26',26)

analyse_data(df_27,'S27',27)

analyse_data(df_28,'S28',28)

train_randomforestmodel_withBVP(df_28, 'S28',28)

analyse_data(df_29,'S29',29)

analyse_data(df_30,'S30',30)

analyse_data(df_31,'S31',31)

analyse_data(df_32,'S32',32)

analyse_data(df_33,'S33',33)

analyse_data(df_34,'S34',34)

analyse_data(df_35,'S35',35)

print (data)

df = pd.DataFrame(data)
#df_score = pd.DataFrame(data['Accuracy Scores'])
#print (df_score)
pivot_df = pd.pivot_table(df, values=['MSE', 'F1 Score', 'Precision', 'Accuracy','Accuracy Scores','Mean Accuracy'], index=['ID','Subject_ID'])
print(pivot_df)
pivot_df.to_csv('Performance_Evaluation.csv')